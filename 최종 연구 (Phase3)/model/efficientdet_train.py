# -*- coding: utf-8 -*-
"""efficientdet_train.ipynb의 사본

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15Jwa4l6MPRsLFpbm3bLmenwzZV_0GXNV
"""

import argparse

parser = argparse.ArgumentParser(description='Argument 사용법')

parser.add_argument('--project_path', help='프로젝트 경로')
parser.add_argument('--name', type=str, help='모델 이름')
parser.add_argument('--batch_size', type=int , default=4 ,help='배치 사이즈')
parser.add_argument('--epoch', type=int , default=100 ,help='에포크 수')

arg = parser.parse_args()

import os
import sys
import tensorflow.compat.v1 as tf

sys.path.append(os.path.join(arg.project_path, 'automl/efficientdet/.'))

import hparams_config
from tf2 import anchors
from model_inspect import ModelInspector

# MODEL = 'efficientdet-d2' 

# def download(m):
#   if m not in os.listdir():
#     !wget https://storage.googleapis.com/cloud-tpu-checkpoints/efficientdet/coco/{m}.tar.gz
#     !tar zxf {m}.tar.gz
#   ckpt_path = os.path.join(os.getcwd(), m)
#   return ckpt_path

# # Download checkpoint.
# ckpt_path = download(MODEL)
# print('Use model in {}'.format(ckpt_path))

class TRAIN_CFG:
  model_name = 'efficientdet-d2' # efficientdet 모델명
  strategy = '' # tpu, 여러개의 GPU들, 단일 GPU 일때 학습 strategy 설정. 
  model_dir = arg.project_path + ''.format(arg.name) # 학습된 모델이 저장될 위치
  pretrained_ckpt = arg.project_path + 'efficientdet-d2' # coco로 pretrained된 checkpoint 파일이 있는 디렉토리 위치
  # num_classes는 8개임. 
  hparams = 'num_classes=8,moving_average_decay=0,mixed_precision=true'
  use_xla = False
  use_fake_data = False
  # max_instances_per_image를 200으로 설정하고, D2 모델이 상대적으로 더 크기 때문에 batch_size를 8로 설정시 GPU Out of Memory 발생 
  batch_size = arg.batch_size
  eval_samples = 500 # evaluation image 데이터 갯수
  steps_per_execution = 1 # train 시 steps 횟수
  num_examples_per_epoch = 2000 # 1 epochs 시 적용하는 examples 개수 
  num_epochs = arg.epoch # epochs 횟수
  train_file_pattern = arg.project_path + 'tfrecord/train/*.tfrecords' # 학습용 tfrecords를 glob 형태로 가져오는 표현식. 
  val_file_pattern = arg.project_path + 'tfrecord/val/*.tfrecords' # 검증용 tfrecords를 glob 형태로 가져오는 표현식. 
  val_json_file = None # optional coco validation json 
  mode = 'traineval' # train만 적용 또는 train과 eval함께 적용(traineval)
  num_shards = 100 # 
  # max_instances_per_image는 기본 100개임. 1개의 이미지에  특정 object의 갯수가 100개를 초과하는 경우가 있음. 200으로 수정. 
  max_instances_per_image = 200
  
  num_cores = 2 # tpu 일때 8 적용.  
  tpu = None
  gcp_project = None
  tpu_zone = None
  eval_master = ''
  eval_name = None
  tf_random_seed = 2021
  profile = False
  debug = False

from tf2.train import setup_model
import hparams_config

import utils
from tf2 import tfmot
from tf2 import train_lib
from tf2 import util_keras

config = hparams_config.get_detection_config(TRAIN_CFG.model_name)
config.override(TRAIN_CFG.hparams)

steps_per_epoch = TRAIN_CFG.num_examples_per_epoch // TRAIN_CFG.batch_size

if tf.config.list_physical_devices('GPU'):
  ds_strategy = tf.distribute.OneDeviceStrategy('device:GPU:0')
else:
  ds_strategy = tf.distribute.OneDeviceStrategy('device:CPU:0')

print(ds_strategy)

params = dict(
      profile=TRAIN_CFG.profile,
      mode = TRAIN_CFG.mode,
      model_name=TRAIN_CFG.model_name,
      steps_per_execution=TRAIN_CFG.steps_per_execution,
      num_epochs = TRAIN_CFG.num_epochs,
      model_dir=TRAIN_CFG.model_dir,
      steps_per_epoch=steps_per_epoch,
      strategy=TRAIN_CFG.strategy,
      batch_size=TRAIN_CFG.batch_size,
      tf_random_seed=TRAIN_CFG.tf_random_seed,
      debug=TRAIN_CFG.debug,
      val_json_file=TRAIN_CFG.val_json_file,
      eval_samples=TRAIN_CFG.eval_samples,
      num_shards=ds_strategy.num_replicas_in_sync,
      # max_instances_per_image는 기본 100개임. 1개의 이미지에  특정 object의 갯수가 100개를 초과하는 경우가 있음. 200으로 수정.
      max_instances_per_image = TRAIN_CFG.max_instances_per_image
      )

config.override(params, True)

# image size를 tuple 형태로 변환. 512는 (512, 512)로 '1920x880' 은 (1920, 880) 으로 변환.  
config.image_size = utils.parse_image_size(config.image_size)
print(config)

import utils
import tensorflow as tf
from tf2 import tfmot
from tf2 import train_lib
from tf2 import util_keras
# P100에서는 적용할 필요 없음. 

precision = utils.get_precision(config.strategy, config.mixed_precision)
policy = tf.keras.mixed_precision.Policy(precision)
tf.keras.mixed_precision.set_global_policy(policy)

import dataloader

def get_dataset(is_training, config):
  # is_training이 True이면 TRAIN_CFG의 train_file_pattern, 그렇지 아니면 val_file_pattern
  file_pattern = (
    TRAIN_CFG.train_file_pattern
    if is_training else TRAIN_CFG.val_file_pattern)
  if not file_pattern:
    raise ValueError('No matching files.')

  return dataloader.InputReader(
    file_pattern,
    is_training=is_training,
    use_fake_data=TRAIN_CFG.use_fake_data,
    max_instances_per_image=config.max_instances_per_image,
    debug=TRAIN_CFG.debug)(
        config.as_dict())

from tf2 import train_lib
from tf2 import train

def get_efficientdet_model(config):
  # 8개의 class를 가진 efficientdet d2 모델을 생성. 
  model = train_lib.EfficientDetNetTrain(config=config)
  model = train.setup_model(model, config)
  # 만약 pretrained 모델이 있으면, 해당 checkpoint weight를 모델로 로딩. 이때 classification layer는 제외. 
  #class TRAIN_CFG: pretrained_ckpt = '/content/efficientdet-d2' 
  if TRAIN_CFG.pretrained_ckpt:
    ckpt_path = tf.train.latest_checkpoint(TRAIN_CFG.pretrained_ckpt)
    # classfication layer를 제외하고 pretrained weight를 생성된 모델로 로딩. 
    util_keras.restore_ckpt(
        model,
        ckpt_path,
        config.moving_average_decay,
        exclude_layers=['class_net'])
    
  train.init_experimental(config)
  return model

model = get_efficientdet_model(config)
model.summary()

print(config.mode)

from tf2 import train
import numpy as np
from tensorflow.keras.callbacks import TensorBoard
import datetime

config.batch_size = arg.batch_size
tr_steps_per_epoch = 3000//config.batch_size
val_steps_per_epoch = 500//config.batch_size
print('tr_steps_per_epoch:', tr_steps_per_epoch, 'val_steps_per_epoch:', val_steps_per_epoch)

log_dir = arg.project_path + "logs/my_board/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)
val_dataset = get_dataset(False, config) if 'eval' in config.mode else None
model.fit(
    get_dataset(True, config),
    epochs=arg.epoch,
    steps_per_epoch=tr_steps_per_epoch,
    callbacks=[train_lib.get_callbacks(config.as_dict(), val_dataset), tensorboard_callback],
    validation_data=val_dataset,
    validation_steps=val_steps_per_epoch)

tf.keras.backend.clear_session()

